{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALOGY\n",
    "## Demonstration Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.name == 'nt':\n",
    "    module_path = os.path.abspath(os.path.join('..\\..\\..'))\n",
    "else:\n",
    "    module_path = os.path.abspath(os.path.join('../../..'))\n",
    "    \n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from analogy import ANALOGY\n",
    "\n",
    "INPUT_FILE_PATH = \"D:\\\\USC\\\\CS548\\\\groupdat\\\\FB15k\\\\\"\n",
    "MODEL_FILE_PATH = INPUT_FILE_PATH\n",
    "\n",
    "TRAIN_FILE_NAME = \"train.txt\"\n",
    "VALIDATION_FILE_NAME = \"valid.txt\"\n",
    "WHOLE_FILE_NAME = \"whole.txt\"\n",
    "TEST_FILE_NAME = \"test.txt\"\n",
    "\n",
    "RELATIONS_FILE_NAME = \"relation2id.txt\"\n",
    "ENTITIES_FILE_NAME = \"entity2id.txt\"\n",
    "\n",
    "MODEL_FILE_NAME = \"analogy.mod\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data Set - This must be run before executing learn_embeddings\n",
    "Validation and Whole Text files are optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = ANALOGY()\n",
    "\n",
    "train_file_names = {\"train\": INPUT_FILE_PATH + TRAIN_FILE_NAME,\n",
    "# Optional          \"valid\": INPUT_FILE_PATH + VALIDATION_FILE_NAME,\n",
    "                    \"whole\": INPUT_FILE_PATH + WHOLE_FILE_NAME,\n",
    "                    \"relations\": INPUT_FILE_PATH + RELATIONS_FILE_NAME,\n",
    "                    \"entities\": INPUT_FILE_PATH + ENTITIES_FILE_NAME}\n",
    "\n",
    "algorithm.read_dataset(train_file_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn Embeddings - Data Set Must be Read First\n",
    "this should take several minutes per epoch if using the time minimizing hyper-parameters below and no filtered evaluation or validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"mode\": 'single',\n",
    "              \"epoch\": 3,\n",
    "              \"batch\": 128,\n",
    "              \"lr\": 0.05,\n",
    "              \"dim\": 40,             # reduced these from 200 to save processing time\n",
    "              \"negative\": 1,         # reduced from 5 to save processing time\n",
    "              \"opt\": 'adagrad',\n",
    "              \"l2_reg\": 0.001,\n",
    "              \"gradclip\": 5,\n",
    "              'filtered': True}     # turned filtered output off to save processing time\n",
    " \n",
    "algorithm.learn_embeddings(parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model - Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm.save_model(MODEL_FILE_PATH + MODEL_FILE_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model - Optional\n",
    "This can load a previously saved model for evaluation or other purposes, not required if current instance of ANALOGY has loaded a dataset and learned the embeddings for that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_alg = ANALOGY()\n",
    "new_alg.load_model(MODEL_FILE_PATH + MODEL_FILE_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Some Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subs = ['/m/08mbj32', '/m/08mbj5d', '/m/08mg_b']\n",
    "test_rels = ['/location/statistical_region/religions./location/religion_percentage/religion',\n",
    "             '/military/military_conflict/combatants./military/military_combatant_group/combatants',\n",
    "             '/award/award_category/winners./award/award_honor/ceremony']\n",
    "test_obs = ['/m/0631_', '/m/0d060g', '/m/01bx35']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Embeddings for sub, rel, object\n",
    "Each entity & relation has 3 embeddings. Two embeddings are used for ComplEx part of Hybrid score measure (one a vector of real numbers and the other a vector of imaginary numbers). The other embedding is used for DistMult part of Hybrid score measure which only has a real number.  \n",
    "\n",
    "Output for each entity/relation retrieval function is tuple with each element having an array for the input list and its embedding, which is outputted as real part of complex number (ComplEx), imaginary part of complex number (Complex), real number (DistMult)  \n",
    "\n",
    "The length of each embedding vector will be equal to the number of dimensions used as a hyper-parameter to the training model divided by 2 (since complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = new_alg.retrieve_entity_embeddings(test_subs)\n",
    "print(\"Length of embedding vector: {}, should equal number of dimensions of model assumption/2\".format(len(subs[0][0])))\n",
    "print(\"Subjects:\")\n",
    "print(subs)\n",
    "\n",
    "rels = new_alg.retrieve_relations_embeddings(test_rels)\n",
    "print(\"Relations:\")\n",
    "print(rels)\n",
    "\n",
    "objs = new_alg.retrieve_entity_embeddings(test_obs)\n",
    "print(\"Objects:\")\n",
    "print(objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Matrix for Test Data\n",
    "Length of each scoring vector (there are 3 for the above test data, one for each s,r,o triplet given) is equal to the number of entities in the vocabulary. For example, for FB15k this is 14,951. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = algorithm.retrieve_scoring_matrix(test_subs, test_rels)\n",
    "print(sm)\n",
    "print(\"Number of test triplets in data: {}\".format(len(sm)))\n",
    "print(\"Length of a scoring vector: {}\".format(len(sm[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Trained Model\n",
    "Produces evaluation metrics for model, whole text file is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_file_names = {\"test\": INPUT_FILE_PATH + TEST_FILE_NAME,\n",
    "                       \"whole\": INPUT_FILE_PATH + WHOLE_FILE_NAME}  # Optional\n",
    "all_res = new_alg.evaluate(evaluate_file_names)\n",
    "for metric in sorted(all_res.keys()):\n",
    "    print('{:20s}: {}'.format(metric, all_res[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
