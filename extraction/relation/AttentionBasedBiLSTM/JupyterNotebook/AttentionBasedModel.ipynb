{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dependencies & files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing required packages - Please uncomment code to install dependencies\n",
    "# !pip install tensorflow\n",
    "# !pip install pandas\n",
    "# !pip install nltk\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing system and going to the required directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing model and making instance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionBasedModel import AttentionBasedBiLstmModel\n",
    "\n",
    "attentionBasedModel = AttentionBasedBiLstmModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset from Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input file\n",
      "[['Although this effect was noted even when cholestyramine was given 4 hours prior to fluvastatin, this regimen did not result in diminished efficacy.', 'cholestyramine', 'drug', '41', '54', 'fluvastatin', 'drug', '83', '93', 'effect'], ['The system as described above has its greatest application in an arrayed configuration of antenna elements.', 'configuration', 'Null', '73', '85', 'elements', 'Null', '98', '106', 'Component-Whole(e2,e1)'], ['Bill Gates is the founder of Microsoft.', 'Bill Gates', 'Person', '0', '9', 'Microsoft', 'organiztion', '29', '37', 'founder']]\n"
     ]
    }
   ],
   "source": [
    "input_file = \"../data/relation_extraction_test_input.txt\"\n",
    "common_format_data = attentionBasedModel.read_dataset(input_file)\n",
    "print(common_format_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model with below paramters..................\n",
      "Max Sentence Length:-  90\n",
      "Dev Sample Percentage:=-  0.1\n",
      "Embedding Path:-  ../res/glove.6B.100d.txt\n",
      "Embedding Dimensions:-  100\n",
      "Dropout probability of embedding layer:-  0.7\n",
      "Dimensionality of RNN hidden:-  100\n",
      "Dropout probability of RNN:-  0.7\n",
      "L2 Regularization lamba:-  1e-5\n",
      "Batch Size:-  10\n",
      "Num Epochs:-  100\n",
      "Display Every:-  10\n",
      "Evaluate Every:-  100\n",
      "Number of Checkpoints:-  5\n",
      "Learning Rate:-  1.0\n",
      "Decay Rate:-  0.9\n",
      "\n",
      "\n",
      "Training Model..................\n",
      "Text Vocabulary Size: 50\n",
      "x = (3, 90)\n",
      "y = (3, 3)\n",
      "\n",
      "\n",
      "Train/Dev split: 2/1\n",
      "\n",
      "Writing to /Users/sonaligujarathi/Desktop/Project/ditk/extraction/relation/AttentionBasedBiLSTM/runs/models\n",
      "\n",
      "Loading glove file ../res/glove.6B.100d.txt\n",
      "Success to load pre-trained word2vec model!\n",
      "\n",
      "2019-05-01T03:10:19.011290: step 10, loss 0.39412, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T03:10:19.139918: step 10, loss 3.91551, acc 0\n",
      "Saved model checkpoint to /Users/sonaligujarathi/Desktop/Project/ditk/extraction/relation/AttentionBasedBiLSTM/runs/models/checkpoints/model-0-10\n",
      "\n",
      "2019-05-01T03:10:19.523306: step 20, loss 0.0305505, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T03:10:19.527611: step 20, loss 5.40546, acc 0\n",
      "2019-05-01T03:10:19.791020: step 30, loss 0.0134481, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T03:10:19.796075: step 30, loss 5.64838, acc 0\n",
      "2019-05-01T03:10:20.044590: step 40, loss 0.0119932, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T03:10:20.048881: step 40, loss 5.91124, acc 0\n",
      "2019-05-01T03:10:20.271760: step 50, loss 0.0161412, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T03:10:20.278118: step 50, loss 5.93644, acc 0\n",
      "2019-05-01T03:10:20.563178: step 60, loss 0.0118569, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T03:10:20.568240: step 60, loss 6.12846, acc 0\n",
      "2019-05-01T03:10:20.803712: step 70, loss 0.0106888, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T03:10:20.808790: step 70, loss 6.26687, acc 0\n",
      "2019-05-01T03:10:21.095166: step 80, loss 0.0102055, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T03:10:21.106021: step 80, loss 6.49254, acc 0\n",
      "2019-05-01T03:10:21.471466: step 90, loss 0.010171, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T03:10:21.476002: step 90, loss 6.59071, acc 0\n",
      "2019-05-01T03:10:21.838080: step 100, loss 0.0101953, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T03:10:21.845238: step 100, loss 6.9411, acc 0\n",
      "\n",
      "\n",
      "Training Completed.................\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Usage : .train(common_format_data,embedding_path =\"glove.6B.100d.txt\",)\n",
    "Compulsory parameter : embedding_path \n",
    "Optional parameters : such as number of epochs,learning rate etc can be given. \n",
    "dev_sample_percentage : Percentage of the training data to use for validation\n",
    "\n",
    "hidden_size : Dimensionality of RNN hidden\n",
    "rnn_dropout_keep_prob : Dropout keep probability of RNN\n",
    "\n",
    "batch_size:Batch Size\n",
    "num_epochs :Number of training epochs\n",
    "display_every :Number of iterations to display training information\n",
    "evaluate_every : Evaluate model on dev set after this many steps\n",
    "num_checkpoints: Number of checkpoints to store\n",
    "learning_rate : Which learning rate to start with\n",
    "\"\"\"\n",
    "attentionBasedModel.train(common_format_data,embedding_path =\"../res/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input file\n",
      "Evaluating Model...............\n",
      "INFO:tensorflow:Restoring parameters from /Users/sonaligujarathi/Desktop/Project/ditk/extraction/relation/AttentionBasedBiLSTM/runs/models/checkpoints/model-0-10\n",
      " Precision: 0.50000000\t    Recall: 0.33333333\t    F1: 0.40000000\n",
      " Precision: 0.50000000\t    Recall: 0.33333333\t    F1: 0.40000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reading test file and sending data to evaluate with help of checkpoint_dir\n",
    "\"\"\"\n",
    "test_file = \"../data/relation_extraction_test_input.txt\"\n",
    "test_common_format_data = attentionBasedModel.read_dataset(test_file)\n",
    "attentionBasedModel.evaluate(test_common_format_data,checkpoint_dir = \"../runs/models/checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Using Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input file\n",
      "Loading model for predicting............\n",
      "INFO:tensorflow:Restoring parameters from /Users/sonaligujarathi/Desktop/Project/ditk/extraction/relation/AttentionBasedBiLSTM/runs/models/checkpoints/model-0-10\n",
      "\n",
      "\n",
      "\n",
      "Sentence :-  Although this effect was noted even when cholestyraminee was given 4 hours prior to fluvastatinn, this regimen did not result in diminished efficacy.\n",
      "Entity 1 :-  cholestyramine\n",
      "Entity 2 :-  fluvastatin\n",
      "Predicted Relation :-  effect\n",
      "Sentence :-  The system as described above has its greatest application in an arrayed configurationn of antenna elements.\n",
      "Entity 1 :-  configuration\n",
      "Entity 2 :-  elements\n",
      "Predicted Relation :-  effect\n",
      "Sentence :-  Bill Gatess is the founder of Microsoftt.\n",
      "Entity 1 :-  Bill Gates\n",
      "Entity 2 :-  Microsoft\n",
      "Predicted Relation :-  founder\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reading predict file in common format and evaluate with help of checkpoint_dir\n",
    "\"\"\"\n",
    "predict_file = \"../data/relation_extraction_test_input.txt\"\n",
    "predict_common_format_data = attentionBasedModel.read_dataset(test_file)\n",
    "predictions = attentionBasedModel.predict(predict_common_format_data,checkpoint_dir = \"../runs/models/checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ../saved_model_dir\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save Model at given location\n",
    "\"\"\"\n",
    "file_save_model = \"../saved_model_dir\"\n",
    "attentionBasedModel.save_model(file_save_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/sonaligujarathi/Desktop/Project/ditk/extraction/relation/AttentionBasedBiLSTM/runs/models/checkpoints/model-0-10\n",
      "<tensorflow.python.client.session.Session object at 0x129848588>\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.text.VocabularyProcessor object at 0x129a67c50>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x128335d30>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.framework.ops.Graph at 0x128335d30>,\n",
       " <tensorflow.python.client.session.Session at 0x129848588>,\n",
       " <tensorflow.contrib.learn.python.learn.preprocessing.text.VocabularyProcessor at 0x129a67c50>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loading Model from file\n",
    "\"\"\"\n",
    "attentionBasedModel.load_model(file_save_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
